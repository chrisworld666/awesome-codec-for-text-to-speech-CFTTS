# 1. Codec
## *1.1 Comparison of Hybrid and Hidden Markov Model(HMM) or Conditional Random Field(CRF)--traditional ways
- 2024.7.25 On the Effect of Purely Synthetic Training Data for Different Automatic Speech Recognition Architectures[[paper](https://arxiv.org/abs/2407.17997)][[code](https://github.com/rwth-i6/returnn-experiments/tree/master/2024-pure-synthetic-data)]
- 2024.2.20 Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition[[paper](https://arxiv.org/abs/2402.13004)]
- 





## 1.2 Deep Learning and Deep Neural Networks(DNN)--new ways(nncodec)
- 2024.7.22 EMO-Codec: A Depth Look at Emotion Preservation Capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations[[paper](https://arxiv.org/abs/2407.15458)]
- 2024.6.11 Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation[[paper](https://arxiv.org/abs/2406.07422)][[code](https://kkksuper.github.io/Single-Codec)]
- 2024.6.8 VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers[[paper](https://arxiv.org/abs/2406.05370)][[code](https://aka.ms/valle2)]
- 2024.5.7 HILCodec: High Fidelity and Lightweight Neural Audio Codec[[paper](https://arxiv.org/abs/2405.04752)]
- 2024.4.28 基于神经网络的低码率语音编码技术研究综述[[paper](https://kns.cnki.net/kcms2/article/abstract?v=wRD08hUPYgxIQfVIo03qBOmF-KFe1SjsE8tlpEc5OvnGr0I4pNML3ML4vI6BI3Q4dCtxdAc-F2rtQBJVg259do_ultMvCkimLUC52KEGwfPT-ztkpwkZ9W9jeZPcb_ImA2ISCuuBEdeOAOpUlLReOAkWvzMob0h9&uniplatform=NZKPT&language=CHS)]
- 2024.4.14 SuperCodec: A Neural Speech Codec with Selective Back-Projection Network[[paper](https://ieeexplore.ieee.org/document/10447744)]
- 2024.4.14 ScoreDec: A Phase-Preserving High-Fidelity Audio Codec with a Generalized Score-Based Diffusion Post-Filter[[paper](https://ieeexplore.ieee.org/document/10448371)]
- 2024.4.14 LightCodec: A High Fidelity Neural Audio Codec with Low Computation Complexity[[paper](https://ieeexplore.ieee.org/document/10447532)]
- 2024.3.25 VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild[[paper](https://arxiv.org/abs/2403.16973)][[code](https://jasonppy.github.io/VoiceCraft_web)]
- 2024.3.18 M2BART: Multilingual and Multimodal Encoder-Decoder Pre-Training for Any-to-Any Machine Translation[[paper](https://ieeexplore.ieee.org/document/10446620)]
- 2024.3.5 NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models[[paper](https://arxiv.org/abs/2403.03100)][[code](https://aka.ms/speechresearch)]
- 2024.2.19 Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models[[paper](https://arxiv.org/abs/2402.12208)][[code](https://github.com/jishengpeng/languagecodec)]
- 2024.2.16 APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding[[paper](https://arxiv.org/abs/2402.10533)]
- 2024.2.2 An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec[[paper](https://arxiv.org/abs/2402.01271)]
- 2023.10.17 A High Fidelity and Low Complexity Neural Audio Coding[[paper](https://arxiv.org/abs/2310.10992)][[code](https://github.com/google/lyra)]
- 2023.9.25 NoLACE: Improving Low-Complexity Speech Codec Enhancement Through Adaptive Temporal Shaping[[paper](https://arxiv.org/abs/2309.14521)][[code](https://282fd5fa7.github.io/NoLACE)][[code](https://gitlab.xiph.org/xiph/opus/-/tree/icassp2024)]
- 2023.9.15 A High-Rate Extension to Soundstream[[paper](https://ieeexplore.ieee.org/document/10248100)]
- 2023.9.14 M3-AUDIODEC: Multi-channel multi-speaker multi-spatial audio codec[[paper](https://arxiv.org/abs/2309.07416)][[code](https://anton-jeran.github.io/MAD/)]
- 2023.5.26 AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec[[paper](https://arxiv.org/abs/2305.16608)][[code](https://github.com/facebookresearch/AudioDec)]
- 2023.5.25 VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation[[paper](https://arxiv.org/abs/2305.16107)]
- 2023.3.23 LMCodec: A Low Bitrate Speech Codec With Causal Transformer Models[[paper](https://arxiv.org/pdf/2303.12984)][[code](https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec)]
- 2022.10.24 High Fidelity Neural Audio Compression[[paper](https://arxiv.org/abs/2210.13438)][[code](https://github.com/facebookresearch/encodec)][[code](https://ai.honu.io/papers/encodec/samples.html)]
- 2022.1.7 NESC: Robust Neural End-2-End Speech Coding with GANs[[paper](https://arxiv.org/abs/2207.03282)]
- 2021.7.7 SoundStream: An End-to-End Neural Audio Codec[[paper](https://arxiv.org/abs/2107.03312)]

# 2 Application of nncodec in Text-to-speech
- 2024.7.22 Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models[[paper](https://arxiv.org/abs/2407.15641)]
- 2024.7.3 Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations[[paper](https://arxiv.org/abs/2407.03495)][[code](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/audio_codec_16khz_small)][[code](https://github.com/NVIDIA/NeMo)]
- 2024.6.27 Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study[[paper](https://arxiv.org/abs/2406.18862)][[code](https://huggingface.co/TencentGameMate/chinese-hubert-large)][[code](https://github.com/chenpk00/IS2024_stream_decoder_only_asr)]
- 2024.6.22 TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers[[paper](https://arxiv.org/abs/2406.15752)][[code](https://ereboas.github.io/TacoLM/)]
- 2024.6.12 VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment[[paper](https://arxiv.org/abs/2406.07855)][[code](https://aka.ms/valler)]
- 2022.4.16 基于编解码器生成对抗网络的CT去噪[[paper](https://kns.cnki.net/kcms2/article/abstract?v=wRD08hUPYgyNeWLqxsgErefvFIq6eitS9VAYSDvSMID-X-8i7d8QgosYK_xxmkhVHWtatWsgWYB3Pmfd5GhX87IVjXBbSsYnFKrgPMGHfQAIraIf6xo1A82BN5f8rSBokv3Ksc6By9l70Krss0vbCdgaJCT4qThy&uniplatform=NZKPT&language=CHS)]
- 2024.6.11 CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems[[paper](https://arxiv.org/abs/2406.07237)][[code](https://codecfake.github.io)]
- 2024.6.3 ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and Zero-shot Language Style Control With Decoupled Codec[[paper](https://arxiv.org/abs/2406.01205)][[code](https://github.com/jishengpeng/ControlSpeech)]
- 2024.3.20 Multi-speaker Text-to-speech Training with Speaker Anonymized Data[[paper](https://arxiv.org/abs/2405.11767)][[code](https://unilight.github.io/Publication-Demos/publications/sa-tts-spl/index.html)]
- 2024.3.18 EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning[[paper](https://ieeexplore.ieee.org/document/10446672)][[code](https://huggingface.co/spaces/enclap-team/enclap)]
- 2024.2.20 Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition[[paper](https://arxiv.org/abs/2402.13004)]
- 2024.1.31 EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning[[paper](https://arxiv.org/abs/2401.17690)][[code](https://github.com/jaeyeonkim99/EnCLAP)]
- 2024.1.21 SingFake: Singing Voice Deepfake Detection[[paper](https://arxiv.org/abs/2309.07525)][[code](https://www.singfake.org/)]
- 2024.1.18 神经网络在语音合成与语音表情识别中的应用[[paper](https://blog.csdn.net/universsky2015/article/details/136011471)]
- 2024.1.14 ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering[[paper](https://arxiv.org/abs/2401.07333)][[code](https://ereboas.github.io/ELLAV/)]
- 2023.9.14 EnCodecMAE: Leveraging neural codecs for universal audio representation learning[[paper](https://arxiv.org/abs/2309.07391)][[code](https://github.com/habla-liaa/encodecmae)]
- 2023.6.12 HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models[[paper](https://arxiv.org/abs/2306.06814)][[code](https://jisang93.github.io/hiddensinger-demo/)]
- 2023.1.5 Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers[[paper](https://arxiv.org/abs/2301.02111)][[code](https://github.com/microsoft/unilm)]
- 2022.3.3 Decoding Knowledge Transfer for Neural Text-to-Speech Training[[paper](https://ieeexplore.ieee.org/document/9767637)][[code](https://creativecommons.org/licenses/by/4.0/)]
- 2022.3.2 MIST-Tacotron: End-to-End Emotional Speech Synthesis Using Mel-Spectrogram Image Style Transfer[[paper](https://ieeexplore.ieee.org/document/9726166)][[code](https://creativecommons.org/licenses/by/4.0/)]












# Reference
- https://github.com/seungwonpark/awesome-tts-samples
- https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers#Speaker-Verification
- https://github.com/xcmyz/speech-synthesis-paper
